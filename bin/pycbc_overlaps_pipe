#! /usr/bin/env python

import os, sys, stat
import tempfile
import shutil
import copy
import warnings
import subprocess
import numpy
import ConfigParser
from optparse import OptionParser

from glue.ligolw import utils
from glue.ligolw import table
from glue.ligolw import lsctables
from glue.ligolw import ligolw
from glue.ligolw.utils import process
from glue import pipeline

from pycbc.overlaps import create_injections, overlap_utils

__prog__ = 'pycbc_overlaps_pipe'
__author__ = 'Collin Capano <collin.capano@ligo.org>'
__description__ = 'Writes overlap dag and sub files.'

# for reading xmldocs
class LIGOLWContentHandler(ligolw.LIGOLWContentHandler):
    pass
lsctables.use_in(LIGOLWContentHandler)

# for writing xmldocs
def create_xmldoc(tables):
    """
    Creates an xmldoc from the list of given LIGOLW tables.
    """
    xmldoc = ligolw.Document()
    xmldoc.appendChild(ligolw.LIGO_LW())
    for table in tables:
        xmldoc.childNodes[0].appendChild(table)
    return xmldoc


def get_injtmpltDB_filename(output_directory, ifo, tag):
    """
    Function to return the combined injection/tmpltbank database file.
    """
    if ifo is None:
        ifo = 'ND'
    return '%s/%s-INJECTIONS_TMPLTBANK%s.sqlite' %( output_directory, ifo, tag.startswith('-') and tag or '_'+tag)


def set_standard_params(job, jobexec, dagbasename):
    job.add_condor_cmd('getenv', 'True')
    job.set_stdout_file('logs/' + os.path.basename(jobexec) + \
        '-$(cluster)-$(process).out')
    job.set_stderr_file('logs/' + os.path.basename(jobexec) + \
        '-$(cluster)-$(process).err')
    job.set_sub_file('%s.%s.sub' %(os.path.basename(jobexec), dagbasename))


def add_opts_from_ini(cp, section, job):
    for option in cp.options(section):
        arg = cp.get(section, option)
        if arg == '':
            arg = None
        job.add_opt(option, arg)


def construct_cmd_from_ini(cp, section, prog):
    args = ['--%s %s' %(option, cp.get(section, option)) \
        for option in cp.options(section)]
    return '%s %s'%(prog, ' '.join(args))

    
parser = OptionParser(description = __description__, usage = '%s [options]' % __prog__)

parser.add_option('-l', '--log-path', help = 'Location to put log directory. Must not be a NSF mounted file system.')
parser.add_option('-t', '--node-local-dir', help = "User's local node directory on which to do database work.")
parser.add_option('-u', '--user-tag', help = 'Set a user tag to be applied to the dag.')
parser.add_option('-c', '--config-file', help = 'Get configuration settings from an ini file.')
parser.add_option('-s', '--start-seed', type = int, help = 'Start seed for the random number generators. Each node that uses a seed argument will increment this by 1.')
parser.add_option('-f', '--tmpltbank-file', help = 'Template bank file to use.')

opts, _ = parser.parse_args()

if opts.start_seed is None:
    raise ValueError, "start seed required"
seed = opts.start_seed
if not os.path.exists(opts.log_path):
    raise ValueError, "log path not found"
if not os.path.exists(opts.config_file):
    raise ValueError, "config file not found"
if opts.node_local_dir is None:
    raise ValueError, 'node-local-dir required'
if opts.tmpltbank_file is None:
    raise ValueError, "tmpltbank-file required"

# get ini file
print "Parsing config-file..."
cp = ConfigParser.ConfigParser()
cp.read(opts.config_file)
# create log file and directory
tempfile.tempdir = opts.log_path
basename = '%s%s' %( 'overlaps', opts.user_tag is not None and \
    '-'+opts.user_tag or '' )
fh, logfile = tempfile.mkstemp(suffix='%s.dag.log' %(basename))
f = os.fdopen(fh, 'w')
f.close()

dag = pipeline.CondorDAG(logfile)
dag.set_dag_file(basename)

# redirect subprocess calls to a temp file
tempfile.tempdir = '.'
fh, tempout = tempfile.mkstemp(suffix='.out')
tempoutf = os.fdopen(fh, 'w')

# set up pipeline parameters
if not os.path.exists('logs/'):
    os.mkdir('logs/')
injections_dir = cp.get('pipeline', 'injections-directory')
if not os.path.exists(injections_dir):
    os.mkdir(injections_dir)
inputdbs_dir = cp.get('pipeline', 'inputdbs-directory')
if not os.path.exists(inputdbs_dir):
    os.mkdir(inputdbs_dir)
results_dir = cp.get('pipeline', 'results-directory')
if not os.path.exists(results_dir):
    os.mkdir(results_dir)
if not os.path.exists('executables'):
    os.mkdir('executables')

# we need to know the ifo for file-naming purposes
if cp.has_option('common', 'ifo'):
    ifo = cp.get('common', 'ifo')
else:
    ifo = None

universe = cp.get('condor', 'universe')

# get executables and set up job for each
create_inj_exec = cp.get('condor', 'create_inj')
combine_dbs_exec = cp.get('condor', 'combine_databases')
randDists = cp.get('condor', 'randomize_distances')
overlaps_exec = cp.get('condor', 'overlaps')
exval_exec = cp.get('condor', 'calc_exval')

# check that the executables exist
execs = [create_inj_exec, combine_dbs_exec, randDists, overlaps_exec,
    exval_exec]
for prog in execs:
    if not os.path.exists(prog):
        raise ValueError, 'executable %s not found' % prog
    # copy to executables directory
    shutil.copyfile(prog, 'executables/%s' %(os.path.basename(prog)))
    shutil.copymode(prog,  'executables/%s' %(os.path.basename(prog)))

# point the create_inj and combine_dbs to the the copy in the executables
# directory
create_inj_exec = 'executables/%s' %(os.path.basename(create_inj_exec))
combine_dbs_exec = 'executables/%s' %(os.path.basename(combine_dbs_exec))

randDists_job = pipeline.CondorDAGJob(universe, 'executables/%s' %(
    os.path.basename(randDists)))
overlaps_job = pipeline.CondorDAGJob(universe, 'executables/%s' %(
    os.path.basename(overlaps_exec)))
exval_job = pipeline.CondorDAGJob(universe, 'executables/%s' %(
    os.path.basename(exval_exec)))

jobs = [(randDists, randDists_job), (overlaps_exec, overlaps_job),
    (exval_exec, exval_job)]

# set standard job parameters 
for jobexec, job in jobs:
    set_standard_params(job, jobexec, basename)


#
#   Set static options
#
if cp.has_option('common', 'psd-model') and \
        cp.has_option('common', 'asd-file'):
    raise ValueError('must provide a psd-model or an asd-file, not both')
elif cp.has_option('common', 'psd-model'):
    psd_type = 'psd-model'
elif cp.has_option('common', 'asd-file'):
    psd_type = 'asd-file'
else:
    raise ValueError('please provide a psd-model or an asd-file in [common]')

# randomize distances
add_opts_from_ini(cp, 'randomize_distances', randDists_job)
# set common options; we only need some of the ones from [common]
common_opts = [psd_type, 'waveform-f-min', 'overlap-f-min'] 
for opt in common_opts:
    randDists_job.add_opt(opt, cp.get('common', opt))
randDists_job.add_opt('tmp-space', opts.node_local_dir)

# overlaps
add_opts_from_ini(cp, 'overlaps', overlaps_job)
# add all arguments from common section
add_opts_from_ini(cp, 'common', overlaps_job)
overlaps_job.add_opt('tmp-space', opts.node_local_dir)
overlaps_job.add_opt('output-dir', results_dir)

# expectation values
add_opts_from_ini(cp, 'calc_exval', exval_job)
# add all arguments from common section
add_opts_from_ini(cp, 'common', exval_job)
exval_job.add_opt('tmp-space', opts.node_local_dir)
exval_job.add_opt('output-dir', results_dir)

#
#
#       Write nodes
#
#

# We will break the dag up by the desired number of injections
# per node, with one thread for each injection set
num_injections = int(cp.get('pipeline', 'num-injections'))
injections_per_node = int(cp.get('pipeline', 'injections-per-node'))
num_inj_nodes = int(numpy.ceil(float(num_injections) / injections_per_node))
remainder = num_injections % injections_per_node

#
#   Run the create_inj program to create the injections
#
inj_file_tag = '%sALLINJPTS_%i' %(opts.user_tag is not None and \
    opts.user_tag+'_' or '', seed)
inj_outfile = create_injections.get_outfilename(injections_dir, 'HL',
    user_tag=inj_file_tag, gz=True)
inj_cmd = construct_cmd_from_ini(cp, 'create_inj', create_inj_exec)
# FIXME: This should be agnostic about what injection code is used,
# but because of the way inspinj deals with time steps and number
# of injections, I am putting an exception here
if create_inj_exec.endswith('lalapps_inspinj'):
    inspinj_start_time = float(cp.get('create_inj', 'gps-start-time'))
    inspinj_time_step = float(cp.get('create_inj', 'time-step')) 
    inspinj_end_time = int(inspinj_start_time + \
        num_injections*inspinj_time_step)
    inj_cmd += ' --write-compress'
    inj_cmd += ' --seed %i --gps-end-time %i --output %s --f-lower %f' %(
        seed, inspinj_end_time, inj_outfile,
        float(cp.get('common', 'waveform-f-min')))
else:
    inj_cmd += ' --output-dir %s --seed %i --n-points %i --user-tag %s' %(
        injections_dir, seed, num_injections, inj_file_tag)
res = subprocess.call(inj_cmd.split(), stdout=tempoutf, stderr=tempoutf)
if res != 0:
    raise ValueError, "Call to %s failed. Command call:\n%s" %(
        create_inj_exec, inj_cmd)

# load the resulting output file
injdoc = utils.load_filename(inj_outfile, contenthandler=LIGOLWContentHandler,
    gz=True)
all_injections = table.get_table(injdoc, 'sim_inspiral')
# sort by the given parameter if desired
if cp.has_option('pipeline', 'sort-injections-by'):
    sort_param = cp.get('pipeline', 'sort-injections-by')
    all_injections.sort(key=lambda x: getattr(x, sort_param))
# get the other tables that are in the document
injdoc_table_names = [y.Name for y in injdoc.childNodes[0].getElements(
    lambda x: x.tagName == ligolw.Table.tagName) if y.Name != u'sim_inspiral']
injdoc_tables = [table.get_table(injdoc, x) for x in injdoc_table_names]

# create the combineDBs command templates; this will be run once per thread
dbs_cmd = construct_cmd_from_ini(cp, 'combine_databases', combine_dbs_exec) 
# add thread-specific arguments to the dbs command
dbs_cmd += ' --database %s %s'
# create a database of the template bank; for each thread, we will just copy
# this database then add the injection file
tempfile.tempdir = '.' 
fh, tmp_db = tempfile.mkstemp(suffix='.sqlite')
f = os.fdopen(fh, 'w')
f.close()
tmp_db_cmd = dbs_cmd %(tmp_db, opts.tmpltbank_file)
# create it
res = subprocess.call(tmp_db_cmd.split(), stdout=tempoutf, stderr=tempoutf)
if res != 0:
    raise ValueError, "Call to %s failed. Command call:\n%s" %(
        combine_dbs_exec, tmp_db_cmd)
# set read permissions
st = os.stat(tmp_db)
os.chmod(tmp_db, st.st_mode | stat.S_IRGRP | stat.S_IROTH)

print "Creating threads..."
exval_nodes = []
for nn in range(num_inj_nodes):

    print "%i / %i\r" %(nn+1, num_inj_nodes),
    sys.stdout.flush()

    thread_tag = '%s%i' %(opts.user_tag is not None and opts.user_tag+'-' or \
        '', nn)

    #
    #   Create a copy of the injections file that only contains this block
    #   of injections, along with the other tables in the xml document
    #
    simtable = lsctables.New(lsctables.SimInspiralTable)
    kstart = nn * injections_per_node
    if nn+1 == num_inj_nodes and remainder != 0:
        kend = kstart + remainder
    else:
        kend = kstart + injections_per_node
    map(simtable.append, all_injections[kstart:kend])
    xmldoc = create_xmldoc(injdoc_tables + [simtable]) 
    inj_outfile = create_injections.get_outfilename(injections_dir, 'HL',
        user_tag=thread_tag, gz=True)
    utils.write_filename(xmldoc, inj_outfile, gz=True)

    #
    #   Create a database with the injections and the templates
    #
    db_outfile = get_injtmpltDB_filename(inputdbs_dir, ifo, thread_tag)
    # copy the temp database containing the template bank to the right name
    shutil.copyfile(tmp_db, db_outfile)
    shutil.copymode(tmp_db, db_outfile)

    # the order of arguments fed to the dbs_cmd argument is output database,
    # injection file (see above)
    this_cmd = dbs_cmd %(db_outfile, inj_outfile)
    res = subprocess.call(this_cmd.split(), stdout=tempoutf, stderr=tempoutf)
    if res != 0:
        raise ValueError, "Call to %s failed. Command call:\n%s" %(
            combine_dbs_exec, this_cmd)

    #
    #   Create node to randomize the distances
    #
    randDists_node = pipeline.CondorDAGNode(randDists_job)
    seed += 1
    randDists_node.add_var_opt('seed', seed)
    #randDists_node.add_var_opt('user-tag', thread_tag)
    randDists_node.add_file_arg(db_outfile)
    randDists_node.add_output_file(db_outfile)
    randDists_node.set_category('randomize_distance')
    dag.add_node(randDists_node)
    last_node = randDists_node

    #
    #   Create node to calculate the overlaps
    #
    overlaps_node = pipeline.CondorDAGNode(overlaps_job)
    seed += 1
    this_tag = '%i%s%s' %(seed, opts.user_tag is None and '-' or '_',
        thread_tag)
    overlaps_node.add_var_opt('user-tag', this_tag)
    overlaps_node.add_file_arg(randDists_node.get_output_files()[0])
    outfile = overlap_utils.get_outfilename(results_dir, ifo,
        user_tag = this_tag,
        num = 0)
    overlaps_node.add_output_file(outfile)
    # remove outfile if it already exists
    if os.path.exists(outfile):
        warnings.warn('Deleting %s' % outfile)
        os.remove(outfile)
    overlaps_node.add_parent(randDists_node)
    overlaps_node.set_category('overlaps')
    dag.add_node(overlaps_node)

    #
    #   Create node to calculate the expectation values
    #
    exval_node = pipeline.CondorDAGNode(exval_job)
    exval_node.add_var_opt('seed', seed)
    this_tag = '%i%s%s' %(seed, opts.user_tag is None and '-' or '_',
        thread_tag)
    exval_node.add_var_opt('user-tag', this_tag)
    exval_node.add_file_arg(overlaps_node.get_output_files()[0])
    outfile = overlap_utils.get_exval_outfilename(results_dir, ifo,
        user_tag = this_tag,
        num = 0)
    exval_node.add_output_file(outfile)
    # remove outfile if it already exists
    if os.path.exists(outfile):
        warnings.warn('Deleting %s' % outfile)
        os.remove(outfile)
    exval_node.add_parent(overlaps_node)
    exval_node.set_category('calc_exval')
    dag.add_node(exval_node)
    exval_nodes.append(exval_node)
print ""

#
#   Create a cache of all output files
#
print "Writing cache files..."
f = open(opts.config_file.replace('.ini', '')+'.cache', 'w')
for outfile in set([outf for node in dag.get_nodes() for outf in \
        node.get_output_files()]):
    print >> f, "%s" % os.path.abspath(outfile)
f.close()
# creaete a second cache of all result databases
f = open(opts.config_file.replace('.ini', '')+'-results.cache', 'w')
for outfile in [outf for node in exval_nodes for outf in \
        node.get_output_files()]:
    print >> f, "%s" % os.path.abspath(outfile)
f.close()

#
#   Write DAG
#
print "Writing dag..."
# set max jobs
if cp.has_option('pipeline', 'max-overlaps-jobs'):
    dag.add_maxjobs_category('overlaps', int(cp.get('pipeline',
        'max-overlaps-jobs')))
if cp.has_option('pipeline', 'max-other-jobs'):
    dag.add_maxjobs_category('add_approximants', int(cp.get('pipeline',
        'max-other-jobs')))
    dag.add_maxjobs_category('randomize_distance', int(cp.get('pipeline',
        'max-other-jobs')))
    dag.add_maxjobs_category('create_inj', int(cp.get('pipeline',
        'max-other-jobs')))
    dag.add_maxjobs_category('combine_databases', int(cp.get('pipeline',
        'max-other-jobs')))
    dag.add_maxjobs_category('calc_exval', int(cp.get('pipeline',
        'max-other-jobs')))

dag.write_sub_files()
dag.write_dag()

# remove the temporary files
os.remove(tmp_db)
tempoutf.close()
os.remove(tempout)

print "Finished!"

sys.exit(0)
